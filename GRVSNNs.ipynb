{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Bo1wg8XZkCZ8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19130,
     "status": "ok",
     "timestamp": 1749871281706,
     "user": {
      "displayName": "hua Yu",
      "userId": "07626386458469956541"
     },
     "user_tz": -180
    },
    "id": "Bo1wg8XZkCZ8",
    "outputId": "8c9f50c0-8117-48d4-9269-1206dba1f731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0oMGg9G3lYyx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7444,
     "status": "ok",
     "timestamp": 1749871291725,
     "user": {
      "displayName": "hua Yu",
      "userId": "07626386458469956541"
     },
     "user_tz": -180
    },
    "id": "0oMGg9G3lYyx",
    "outputId": "29185a00-0760-436a-f068-1e76eddf9098"
   },
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8_cYsJ7ClO0V",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_cYsJ7ClO0V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, KFold # KFold needs to be imported\n",
    "from scipy.stats import pearsonr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Layer, Multiply, Add, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Data\n",
    "loadings_df = pd.read_csv('') #the path for the loadings\n",
    "micedata_df = pd.read_csv('') ##the path for the genomic data\n",
    "\n",
    "# processing\n",
    "micedata_features = micedata_df.iloc[:, 2:].values\n",
    "loadings_real = loadings_df.applymap(lambda x: np.real(complex(x)) if isinstance(x, str) else np.real(x)).values\n",
    "#loadings_real = loadings_df.map(lambda x: np.real(complex(x)) if isinstance(x, str) else np.real(x)).values\n",
    "\n",
    "print(f\"Shape of micedata_features: {micedata_features.shape}\")\n",
    "print(f\"Shape of loadings_real: {loadings_real.shape}\")\n",
    "\n",
    "\n",
    "# Feature Reduction for loadings ONLY\n",
    "loadings_reduced = loadings_real\n",
    "\n",
    "# Combine features\n",
    "combined_features = np.hstack((micedata_features, loadings_reduced))\n",
    "\n",
    "print(f\"Shape of combined_features after concatenation: {combined_features.shape}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(combined_features)\n",
    "target = micedata_df.iloc[:, :2].values\n",
    "\n",
    "# Define the layers\n",
    "class GatedResidualUnit(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(GatedResidualUnit, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dense = Dense(units, activation='elu', kernel_regularizer=l2(0.01))\n",
    "        self.gate = Dense(units, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "        # This layer will project the residual connection if its shape doesn't match 'units'\n",
    "        self.residual_projection = None # Initialize as None, build later if needed\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if input_shape[-1] != self.units:\n",
    "            self.residual_projection = Dense(self.units, activation='linear', kernel_regularizer=l2(0.01))\n",
    "        else:\n",
    "            self.residual_projection = tf.identity # Use identity if shapes match, no projection needed\n",
    "        super(GatedResidualUnit, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        residual = inputs\n",
    "        x = self.dense(inputs)\n",
    "        gate = self.gate(inputs)\n",
    "        gated_output = Multiply()([x, gate])\n",
    "\n",
    "        \n",
    "        if self.residual_projection is not tf.identity: \n",
    "            projected_residual = self.residual_projection(residual)\n",
    "        else:\n",
    "            projected_residual = residual \n",
    "\n",
    "        return Add()([gated_output, projected_residual])\n",
    "\n",
    "class VariableSelectionNetwork(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(VariableSelectionNetwork, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dense = Dense(units, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if inputs.shape[1] < self.units:\n",
    "            inputs_for_selection = inputs\n",
    "        else:\n",
    "            inputs_for_selection = inputs[:, :self.units]\n",
    "\n",
    "        dense_output = self.dense(inputs_for_selection)\n",
    "        hard_sigmoid_output = tf.maximum(0.0, tf.minimum(1.0, 0.2 * dense_output + 0.5))\n",
    "        return Multiply()([inputs_for_selection, hard_sigmoid_output])\n",
    "\n",
    "# creat the grvsnn model\n",
    "def create_grn_vsn_model(input_shape, dropout_rate=0.3, n_hidden_units = 128):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    #step 1: GR block\n",
    "    grn_output = GatedResidualUnit(units = n_hidden_units)(inputs)\n",
    "    grn_output = Dropout(dropout_rate)(grn_output)\n",
    "\n",
    "    #step 2: VS block\n",
    "    selected_features = VariableSelectionNetwork(units = n_hidden_units)(grn_output)\n",
    "    outputs = Dense(2)(selected_features)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# BO\n",
    "def train_model(lr, dropout_rate):\n",
    "    lr = max(0.00001, min(lr, 0.001))\n",
    "    dropout_rate = max(0.1, min(dropout_rate, 0.5))\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42) #n_splits can be 5 or 10. in our case, we use 10-fold\n",
    "    mse_scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(normalized_features):\n",
    "        X_train, X_val = normalized_features[train_index], normalized_features[val_index]\n",
    "        y_train, y_val = target[train_index], target[val_index]\n",
    "\n",
    "        model = create_grn_vsn_model(input_shape=(normalized_features.shape[1],), dropout_rate=dropout_rate)\n",
    "\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "        loss, _ = model.evaluate(X_val, y_val, verbose=0)\n",
    "        mse_scores.append(loss)\n",
    "\n",
    "    return -np.mean(mse_scores)\n",
    "\n",
    "optimizer = BayesianOptimization(f=train_model, pbounds={'lr': (0.00001, 0.001), 'dropout_rate': (0.1, 0.5)}, verbose=2)\n",
    "optimizer.maximize(init_points=5, n_iter=10)\n",
    "\n",
    "# 8. Training and test\n",
    "best_params = optimizer.max['params']\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "final_mse_scores = []\n",
    "for train_index, val_index in kf.split(normalized_features):\n",
    "    X_train, X_val = normalized_features[train_index], normalized_features[val_index]\n",
    "    y_train, y_val = target[train_index], target[val_index]\n",
    "\n",
    "    final_model = create_grn_vsn_model(input_shape=(normalized_features.shape[1],), dropout_rate=best_params['dropout_rate'])\n",
    "    final_model.compile(optimizer=Adam(learning_rate=best_params['lr']), loss='mean_squared_error',\n",
    "                         metrics=[tf.keras.metrics.MeanSquaredError(name='mse_col1'), tf.keras.metrics.MeanSquaredError(name='mse_col2')])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "    final_model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "    loss, mse_col1, mse_col2 = final_model.evaluate(X_val, y_val)\n",
    "    final_mse_scores.append(loss)\n",
    "    print(f\"Fold test MSE: {loss}\\nTrait 1 test MSE: {mse_col1}\\nTrait 2 test MSE: {mse_col2}\")\n",
    "\n",
    "print(f\"Average Test MSE across 5 folds: {np.mean(final_mse_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
