{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pun7bW7gdA6R"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "id": "SyECd-iXdBmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Layer, Multiply, Add, Dropout, LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "####\n",
        "# If you need to debug custom layers step-by-step, you can re-enable it.\n",
        "# tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# read data\n",
        "loadings_df = pd.read_csv(' ') #the path for the loadings file\n",
        "micedata_df = pd.read_csv(' ') ##the path for the genomic data\n",
        "\n",
        "# data processing\n",
        "micedata_features = micedata_df.iloc[:, 2:].values\n",
        "# using .map for element-wise application\n",
        "loadings_real = loadings_df.map(lambda x: np.real(complex(x)) if isinstance(x, str) else np.real(x)).values\n",
        "\n",
        "print(f\"Shape of micedata_features: {micedata_features.shape}\")\n",
        "print(f\"Shape of loadings_real: {loadings_real.shape}\")\n",
        "\n",
        "# feature Reduction for loadings\n",
        "loadings_reduced = loadings_real\n",
        "\n",
        "# combine Features\n",
        "combined_features = np.hstack((micedata_features, loadings_reduced))\n",
        "\n",
        "print(f\"Shape of combined_features after concatenation: {combined_features.shape}\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "normalized_features = scaler.fit_transform(combined_features)\n",
        "target = micedata_df.iloc[:, :2].values #for the mice data, there are two traits\n",
        "\n",
        "\n",
        "# define the layers\n",
        "class GatedResidualUnit(Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(GatedResidualUnit, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        # f1 = sigma_f(W1 x + b1) -> ELU activation\n",
        "        self.dense_f1 = Dense(units, activation='elu', kernel_regularizer=l2(0.01))\n",
        "\n",
        "        # This layer produces 'h'\n",
        "        self.dense_h = Dense(units, activation='linear', kernel_regularizer=l2(0.01))\n",
        "        # g\n",
        "        self.dense_g = Dense(units, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
        "        # Layer Normalization\n",
        "        self.layer_norm = LayerNormalization()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # residual_projection is needed if input_shape[-1] != self.units\n",
        "        if input_shape[-1] != self.units:\n",
        "            self.residual_projection = Dense(self.units, activation='linear', kernel_regularizer=l2(0.01))\n",
        "        else:\n",
        "            self.residual_projection = tf.identity # Identity function if shapes match\n",
        "        super(GatedResidualUnit, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        ### original input\n",
        "        residual = inputs\n",
        "\n",
        "        # f1 = sigma_f(W1 x + b1)\n",
        "        f1 = self.dense_f1(inputs)\n",
        "        ### For thie model, following the non-linear transformation, a linear layer with dropout is applied to prevent overfitting that yields an output f2\n",
        "        # We will assume f1 directly serves as f2 for simplicity, as explicit linear+dropout layer is not in original code for f2.\n",
        "        # If you want to add an explicit linear+dropout for f2, it would go here.\n",
        "        f2 = f1 # assuming f1 acts as f2\n",
        "\n",
        "        # h\n",
        "        h = self.dense_h(f2)\n",
        "\n",
        "        # g\n",
        "        g = self.dense_g(f2)\n",
        "\n",
        "        # z: element-wise multiplication\n",
        "        gated_h = Multiply()([h, g])\n",
        "        # Ensure residual is projected/reshaped before (1-g) multiplication if necessary\n",
        "        projected_residual = self.residual_projection(residual)\n",
        "        gated_residual = Multiply()([projected_residual, (1 - g)])\n",
        "\n",
        "        z = Add()([gated_h, gated_residual])\n",
        "\n",
        "        # hat_z = LayerNorm(z)\n",
        "        hat_z = self.layer_norm(z)\n",
        "\n",
        "        return hat_z\n",
        "\n",
        "class VariableSelectionNetwork(Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(VariableSelectionNetwork, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "        # Softmax-based Feature Selection\n",
        "        # We produce raw scores here, then apply softmax.\n",
        "        self.dense_scores = Dense(units, activation='linear', kernel_regularizer=l2(0.01))\n",
        "\n",
        "        # --- Hard-Sigmoid based Feature Selection (Commented out, for alternative use) ---\n",
        "        # The paper also mentions \"incorporate the hard-sigmoid function to accelerate convergence\n",
        "        # and enhance sparsity\". If you want to use this, uncomment the lines below and in call().\n",
        "        # self.dense_hard_sigmoid_scores = Dense(units, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs here are hat_z from GRU, which the paper refers to as hat_z_i\n",
        "        inputs_for_selection = inputs # Renamed for clarity, as per paper's hat_z_i\n",
        "\n",
        "        # --- Softmax-based Feature Selection (active) ---\n",
        "        # Get raw scores for feature importance\n",
        "        raw_scores = self.dense_scores(inputs_for_selection)\n",
        "\n",
        "        # Apply softmax to get weights w_i\n",
        "        # w_i = e^(raw_score_i) / sum(e^(raw_score_j))\n",
        "        weights_w_i = tf.nn.softmax(raw_scores, axis=-1) # Apply softmax along the feature dimension\n",
        "\n",
        "        selected_features = Multiply()([inputs_for_selection, weights_w_i])\n",
        "\n",
        "        # --- Hard-Sigmoid based Feature Selection (Commented out) ---\n",
        "        # If you want to use the hard-sigmoid for feature selection:\n",
        "        # 1. Uncomment the 'self.dense_hard_sigmoid_scores' in __init__.\n",
        "        # 2. Uncomment the following block and comment out the softmax block above.\n",
        "        # 3. Adjust the feature importance calculation in the training loop accordingly.\n",
        "        #\n",
        "        # # Ensure inputs_for_selection matches self.units dimensions if truncated\n",
        "        # # This block was part of the hard-sigmoid implementation you provided\n",
        "        # # if inputs.shape[1] < self.units:\n",
        "        # #     inputs_for_selection_hard_sigmoid = inputs\n",
        "        # # else:\n",
        "        # #     inputs_for_selection_hard_sigmoid = inputs[:, :self.units]\n",
        "        #\n",
        "        # # dense_output_hard_sigmoid = self.dense_hard_sigmoid_scores(inputs_for_selection_hard_sigmoid)\n",
        "        # # hard_sigmoid_output = tf.maximum(0.0, tf.minimum(1.0, 0.2 * dense_output_hard_sigmoid + 0.5))\n",
        "        # # selected_features = Multiply()([inputs_for_selection_hard_sigmoid, hard_sigmoid_output])\n",
        "\n",
        "        return selected_features\n",
        "\n",
        "\n",
        "# create the GRVSNN model\n",
        "def create_grn_vsn_model(input_shape, dropout_rate=0.3, n_hidden_units=128):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Step 1: GR block\n",
        "    grn_output = GatedResidualUnit(units=n_hidden_units)(inputs)\n",
        "    grn_output = Dropout(dropout_rate)(grn_output)\n",
        "\n",
        "    # Step 2: VS block\n",
        "    selected_features = VariableSelectionNetwork(units=n_hidden_units)(grn_output)\n",
        "    # Final output layer for regression: must use linear activation\n",
        "    outputs = Dense(2, activation='linear')(selected_features)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# BO for hyperparameters\n",
        "def train_model(lr, dropout_rate):\n",
        "    lr = max(0.00001, min(lr, 0.001))\n",
        "    dropout_rate = max(0.1, min(dropout_rate, 0.5))\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    mse_scores = []\n",
        "\n",
        "    for train_index, val_index in kf.split(normalized_features):\n",
        "        X_train, X_val = normalized_features[train_index], normalized_features[val_index]\n",
        "        y_train, y_val = target[train_index], target[val_index]\n",
        "\n",
        "        model = create_grn_vsn_model(input_shape=(normalized_features.shape[1],), dropout_rate=dropout_rate, n_hidden_units=128)\n",
        "        optimizer = Adam(learning_rate=lr)\n",
        "        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "        model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "        loss, _ = model.evaluate(X_val, y_val, verbose=0)\n",
        "        mse_scores.append(loss)\n",
        "\n",
        "    return -np.mean(mse_scores)\n",
        "\n",
        "# Bayesian Optimization setup\n",
        "optimizer = BayesianOptimization(f=train_model, pbounds={'lr': (0.00001, 0.001), 'dropout_rate': (0.1, 0.5)}, verbose=2)\n",
        "optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "# Training and test with best parameters\n",
        "best_params = optimizer.max['params']\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "final_mse_scores = []\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(normalized_features)):\n",
        "    X_train, X_val = normalized_features[train_index], normalized_features[val_index]\n",
        "    y_train, y_val = target[train_index], target[val_index]\n",
        "\n",
        "    final_model = create_grn_vsn_model(input_shape=(normalized_features.shape[1],), dropout_rate=best_params['dropout_rate'], n_hidden_units=128)\n",
        "    final_model.compile(optimizer=Adam(learning_rate=best_params['lr']), loss='mean_squared_error',\n",
        "                        metrics=[tf.keras.metrics.MeanSquaredError(name='mse_col1'), tf.keras.metrics.MeanSquaredError(name='mse_col2')])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
        "    final_model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "    loss, mse_col1, mse_col2 = final_model.evaluate(X_val, y_val, verbose=0)\n",
        "    final_mse_scores.append(loss)\n",
        "    print(f\"Fold test MSE: {loss}\\nTrait 1 test MSE: {mse_col1}\\nTrait 2 test MSE: {mse_col2}\")\n",
        "\n",
        "print(f\"\\nAverage Test MSE across 5 folds: {np.mean(final_mse_scores)}\")\n",
        "\n",
        "\n",
        "# optional: feature Importance Calculation\n",
        "\n",
        "try:\n",
        "    vsn_layer = None\n",
        "    for layer in final_model.layers:\n",
        "        if isinstance(layer, VariableSelectionNetwork):\n",
        "            vsn_layer = layer\n",
        "            break\n",
        "\n",
        "    if vsn_layer is not None:\n",
        "        gru_output_layer = None\n",
        "        for i, layer in enumerate(final_model.layers):\n",
        "            if isinstance(layer, GatedResidualUnit):\n",
        "                gru_output_layer = final_model.layers[i].output\n",
        "                break\n",
        "\n",
        "        if gru_output_layer is not None:\n",
        "            grn_model_for_hat_z = Model(inputs=final_model.input, outputs=gru_output_layer)\n",
        "            sample_hat_z = grn_model_for_hat_z.predict(normalized_features[:10])\n",
        "\n",
        "            # since softmax is now active, here we can get raw scores then softmax them\n",
        "            vsn_raw_scores = vsn_layer.dense_scores(sample_hat_z).numpy()\n",
        "            vsn_weights_w_i = tf.nn.softmax(vsn_raw_scores, axis=-1).numpy()\n",
        "\n",
        "            print(\"\\n--- Example Feature Importance (Softmax Weights from VSN) ---\")\n",
        "            print(f\"Average weights (across first 10 samples) for each feature in the VS block output:\\n{np.mean(vsn_weights_w_i, axis=0)}\")\n",
        "\n",
        "            # apply thresholding as mentioned (threshold of 0.05 on z'_i values)\n",
        "            # z'_i is the output of the VSN layer (selected_features)\n",
        "            vsn_output_example = vsn_layer(sample_hat_z).numpy()\n",
        "\n",
        "            threshold = 0.05\n",
        "            num_selected_features_per_sample = np.sum(vsn_output_example >= threshold, axis=1)\n",
        "            print(f\"\\nNumber of features with z'_i >= {threshold} (per sample, first 10 samples):\")\n",
        "            print(num_selected_features_per_sample)\n",
        "        else:\n",
        "            print(\"Could not find GatedResidualUnit layer to extract hat_z.\")\n",
        "    else:\n",
        "        print(\"Could not find VariableSelectionNetwork layer.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WIHGnFmEdBpR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}